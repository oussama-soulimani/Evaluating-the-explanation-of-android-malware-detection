import numpy as np
import pandas as pd

from Lore.LORE import pyyadt
np.random.seed(1)
import sklearn
from sklearn.feature_selection import VarianceThreshold
import csv
from anchor import anchor_tabular, utils
from lime import lime_tabular
from Lore.LORE.gpdatagenerator import calculate_feature_values
from Lore.LORE.util import dataframe2explain, label_encode
from Lore.LORE.neighbor_generator import *
from shap.explainers.treeExp import TreeExplainer
from random import sample
from sklearn.model_selection import train_test_split
from interpret.blackbox import MorrisSensitivity

warnings.filterwarnings("ignore")

def split(apps, labels):
    # print("Splitting data")
    splits = sklearn.model_selection.ShuffleSplit(n_splits=1, test_size=.2,random_state=1)
    train_idx, test_idx = [x for x in splits.split(apps)][0]
    train = apps[train_idx]
    train_labels = labels[train_idx]
    cv_splits = sklearn.model_selection.ShuffleSplit(n_splits=1,test_size=.5,random_state=1)
    ntest_idx = [x for x in cv_splits.split(test_idx)][0][1]

    test_idx = test_idx[ntest_idx]
    test = apps[test_idx]
    test_labels = labels[test_idx]
    return train, train_labels, test, test_labels, train_idx

def selectFeatures(train_apps, test_apps, feature_names,numRelevant=219, numIrrelevant=0):
    # print("Selecting & filter features")
    # p = 0.015
    # sel = VarianceThreshold(threshold=(p * (1 - p)))
    # sel.fit(train_apps)
    variances =np.var(train_apps, axis=0)
    RelevantFeatures = np.sort((np.argsort(variances)[::-1][:numRelevant]))
    # print(RelevantFeatures)
    if numIrrelevant!=0:
        IrrelevantFeatures = np.sort((np.argsort(-variances)[::-1][:numIrrelevant])) # get features with lowest variances
        selected_features = np.sort(np.concatenate((RelevantFeatures, IrrelevantFeatures)))
    else:
        selected_features = RelevantFeatures
    train_apps = np.array(list(map(lambda x:x[selected_features], train_apps)))
    test_apps = np.array(list(map(lambda x:x[selected_features], test_apps)))
    feature_names = feature_names[selected_features]
    return train_apps, test_apps, feature_names, selected_features


def ReformData(feature_names,dataFile, reader,train_idx, test_apps):
    dataset = {}
    dataset["columns"] = np.append( "Label", feature_names)
    dataset['class_name'] = 'Label'
    columns_tmp = list(dataset["columns"])
    columns_tmp.remove(dataset["class_name"])
    dataset['idx_features'] = {i: col for i, col in enumerate(columns_tmp)}
    dataset['discrete'] = dataset["columns"]
    dataset['continuous'] = []
    feature_types = ["integer"]*len(dataset["columns"])
    dataset['features_type'] = dict(zip(dataset["columns"], feature_types))
    df = pd.read_csv(dataFile, delimiter = ",", skipinitialspace=True).drop(list(set(range(len(reader)-1))-set(train_idx)))
    df =df[dataset["columns"]]
    dataset['label_encoder'] = label_encode(df, dataset['discrete'])[1]
    dataset['possible_outcomes'] = ["malware", "benign"]
    dataset["feature_values"] = calculate_feature_values(test_apps,dataset["columns"], 'Label',dataset["columns"], [], 1000, True, False  )
    return dataset

class Explanations:
    def __init__(self, numIrrelevant=0):
        self.dataFile = "./FeaturesPerApp.csv"
        file = open(self.dataFile, "r")
        self.reader = list(csv.reader(file))
        file.close()
        self.AllFeatures = np.array(self.reader[0][1:-1])
        apps = self.reader[1:] # remove column names
        # print("Loading labels")
        for i in range(len(apps)):
            apps[i] =list(map(lambda x:int(x), apps[i][1:])) #remove app names
        self.Allabels = np.array(list(map(lambda x:x[-1], apps))) # label column
        # print("Loading content")
        self.apps = np.array(list(map(lambda x:x[0:-1], apps))) #feature columns
        train_apps, self.train_labels, test_apps,self.test_labels, self.train_idx = split(self.apps, self.Allabels)
        self.train_apps, _ , self.feature_names, self.selected_features = selectFeatures(train_apps, test_apps, self.AllFeatures,numIrrelevant=numIrrelevant)

        with open("./test.csv") as testfile:
            explain_apps = list(csv.reader(testfile))[1:] #rows
            explain_apps = list(map(lambda x:x[1:], explain_apps))
            for i in range(len(explain_apps)):
                explain_apps[i] = list(map(lambda x:int(x), explain_apps[i]))
        self.test_apps = np.array(list(map(lambda x:np.array(x)[self.selected_features], test_apps)))
        self.explain_apps = np.array(list(map(lambda x:np.array(x)[self.selected_features], explain_apps)))
        self.class_names = ["0","1"]
        
    def classify(self, classifier, i=None):
        # print("Create classification")
        classifier.fit(self.train_apps, self.train_labels)
        print('Train', sklearn.metrics.accuracy_score(self.train_labels, classifier.predict(self.train_apps)))
        print('Test', sklearn.metrics.accuracy_score(self.test_labels, classifier.predict(self.test_apps)))
        return classifier

    def LimeExplain(self, Classifier, app, num_features=20):
        explainer = lime_tabular.LimeTabularExplainer(self.train_apps, mode="classification", feature_names=self.feature_names, class_names=self.class_names, discretize_continuous=False)
        # prediction = explainer.class_names[Classifier.predict(np.array(app).reshape(1, -1))[0]]
        # print("prediction: "+str(prediction))
        exp = explainer.explain_instance(np.array(app), Classifier.predict_proba, num_features=num_features,num_samples = 5000)
        explanations = list(map(lambda x:x[0], exp.as_list()))
        return explanations
        # print('Lime Explanation: %s' % (' AND '.join(explanations)))
        
    def AnchorExplain(self, Classifier, app):
        explainer = anchor_tabular.AnchorTabularExplainer(self.class_names, self.feature_names, self.train_apps)
        # prediction = explainer.class_names[Classifier.predict(np.array(app).reshape(1, -1))[0]]
        # print("prediction: "+str(prediction))
        exp = explainer.explain_instance(np.array(app), Classifier.predict, threshold=1)
        explanation = list(map(lambda x:x.split(" ")[0], exp.names()))
        return explanation
        # print('Anchor Explanation: %s' % (' AND '.join(exp.names())))
        # print()
    

    # def LoreExplain(self,Classifier, app):
    #     # print("ReformData")
    #     dataset = ReformData(self.feature_names,self.dataFile, self.reader,self.train_idx, self.test_apps)
    #     ap = np.array([app])
    #     dfZ, x = dataframe2explain(np.array([app]), dataset,0,Classifier)
    #     # print("genetic_neighborhood")
    #     dfZ, Z = genetic_neighborhood(dfZ, x, Classifier, dataset)
    #     # print("fit")
    #     dt, dt_dot = pyyadt.fit(dfZ,dataset['class_name'], dataset['columns'], dataset['features_type'],dataset['discrete'], dataset['continuous'],filename="FeaturesPerApp", path="./", sep=";", log=False)
    #     dfx = build_df2explain(Classifier,x.reshape(1, -1), dataset).to_dict('records')[0]
    #     cc_outcome, rule, tree_path = pyyadt.predict_rule( dt, dfx, dataset["class_name"], dataset["features_type"], dataset["discrete"], dataset["continuous"])
    #     explanations = rule[1].keys()
    #     return explanations
    #     # print()
    
    def MorrisSensitivityExplain(self):
        from sklearn.ensemble import RandomForestClassifier
        apps = self.apps
        df = pd.DataFrame(apps, columns=self.AllFeatures)
        X_train, X_test, y_train, _ = train_test_split(df, self.Allabels, test_size=0.5, random_state=1)
        X_train_list = np.array(X_train.values.tolist())
        X_test_list = np.array(X_test.values.tolist())
        X_train, _ , _, selected_features = selectFeatures(X_train_list, X_test_list, self.AllFeatures,numIrrelevant=50)
        X_train = pd.DataFrame(X_train, columns=self.AllFeatures[selected_features])
       
        blackbox_model = RandomForestClassifier(n_estimators=50, n_jobs=5)
        blackbox_model.fit(X_train, y_train)
        print(len(X_train))
        
        sensitivity = MorrisSensitivity(predict_fn=blackbox_model.predict_proba, data=X_train)
        features=sensitivity.explain_global(name="Global Sensitivity").getFeatures(numExplanations=20)

        return features["names"]   
    def ShapExplain(self,Classifier, app, num_features=20):
        # print("Shap Explanation: ")
        shap_values = np.array(TreeExplainer(Classifier).shap_values(app))
        shap_values=np.delete(shap_values, -1)
        # The n features with the highest shap values are the features that
        # have a positve impact on the prediction
        features_idx = np.argsort(-shap_values)[:num_features]
        explanation =[]
        for i in features_idx:
            if app[i]==1:
                explanation.append(self.feature_names[i])
        return explanation
    
    def EDCExplain(self, Classifier, app): 
        app=[app]
        probs = Classifier.predict_proba(app)[:,1]
        threshold_classifier_probs = np.percentile(probs,75) 
        classification_model = Classifier 

        def classifier_fn(X):
            c=classification_model.predict_proba(X)
            y_predicted_proba=c[:,1]
            return y_predicted_proba
        from sedc_algorithm import SEDC_Explainer
        explainer_SEDC = SEDC_Explainer(feature_names = np.array(self.feature_names), 
                               threshold_classifier = threshold_classifier_probs, 
                               classifier_fn = classifier_fn, max_features=1000)
        index = 0
        instance_idx = app[index]
        explanation = explainer_SEDC.explanation(instance_idx)
        explanation = list(map(lambda x:x[0], explanation))
        return explanation
        # print(explanation)
    
if __name__ == "__main__":
    # try:    
    exp = Explanations(numIrrelevant=0)
    exp.MorrisSensitivityExplain()
    from sklearn.svm import SVC
    classifier = exp.classify(classifier=SVC(probability=True))
    expl = exp.MaceExplain(classifier, exp.explain_apps[0])
    print(expl)
